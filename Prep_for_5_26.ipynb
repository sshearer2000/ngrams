{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Prep for 5-26.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IOGbJoRcSp5P",
        "Zbb2H8a6Sp5d",
        "LeU3ds1zSp5p",
        "2Mr3qU4jSp5y",
        "Z-oyqeeQSp5_",
        "b1cydtt3Sp6L",
        "Vws21wy6Sp6Q",
        "0q5mCcJlSp6W",
        "OAUC8Q6SSp6d",
        "Gha0rKeqt8B8",
        "4rJRiCxZgRQL",
        "NflhA0MX8Sqp",
        "HmZ-Y5vIuix8",
        "prLjqbrXuvk6",
        "y9f9DR3wu10h",
        "8Qu9aXnVu5de"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOGbJoRcSp5P",
        "colab_type": "text"
      },
      "source": [
        "# Import statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTGOCpzPSp5S",
        "colab_type": "code",
        "outputId": "f971008b-8e61-4d32-9e3c-cdb8cbf2fe2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "import re\n",
        "from pandas import DataFrame, Series\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from gensim.models.tfidfmodel import TfidfModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from wordcloud import WordCloud"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbb2H8a6Sp5d",
        "colab_type": "text"
      },
      "source": [
        "# Function to rename long-worded columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyWaLTq8Sp5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rename_col(csv, og_name, new_name):\n",
        "    csv.rename(columns={og_name:new_name},inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeU3ds1zSp5p",
        "colab_type": "text"
      },
      "source": [
        "# Import csv files as dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3Z-_f-lSp5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stud = pd.read_csv('Difficulty Protocol Data - Student Protocol.csv',encoding=\"utf-8\",index_col=0)\n",
        "profta = pd.read_csv('Difficulty Protocol Data - Faculty_TA Protocol.csv',encoding=\"utf-8\",index_col=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Mr3qU4jSp5y",
        "colab_type": "text"
      },
      "source": [
        "# Rename columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcqeRm-qSp5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rename_col(stud,'Timestamp','time')\n",
        "rename_col(stud,'Participant ID','id')\n",
        "rename_col(stud, 'What topics did you cover in class this week?','topics_covered')\n",
        "rename_col(stud,'What kinds of activities did you focus on out of class this week?  [Reading/Research]','reading_research')\n",
        "rename_col(stud, 'What kinds of activities did you focus on out of class this week?  [Continuing/Finishing In-Class Work]','finishing_class_work')\n",
        "rename_col(stud, 'What kinds of activities did you focus on out of class this week?  [Homework Problems/Assignments]','homework')\n",
        "rename_col(stud, 'What kinds of activities did you focus on out of class this week?  [Out-of-Class Labs]', 'out_of_class_labs')\n",
        "rename_col(stud, 'What kinds of activities did you focus on out of class this week?  [Multi-Week Project Experience]','multi_week_project')\n",
        "rename_col(stud, 'What kinds of activities did you focus on out of class this week?  [Other]','other_outside_class')\n",
        "rename_col(stud, 'What concepts/activities did you or your peers struggle most with this week?','struggle_concepts')\n",
        "rename_col(stud, 'What questions did you or your peers raise to your instructor/TAs this week?','questions_raised')\n",
        "rename_col(stud, 'Were there any questions from your peers that surprised you this week?','surprise_questions')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDwfIUYxSp54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rename_col(profta, 'Timestamp','time')\n",
        "rename_col(profta,'Participant ID', 'id')\n",
        "rename_col(profta, 'What is your role in the course you are reflecting on? ','role')\n",
        "rename_col(profta, 'What topics did you cover this week?','topics_covered')\n",
        "rename_col(profta, 'What kinds of activities did your students focus  on this week out of class?  [Reading/Research]','reading_research')\n",
        "rename_col(profta, 'What kinds of activities did your students focus  on this week out of class?  [Continuing/Finishing In-Class Work]','finishing_class_work')\n",
        "rename_col(profta, 'What kinds of activities did your students focus  on this week out of class?  [Homework Problems/Assignments]','homework')\n",
        "rename_col(profta, 'What kinds of activities did your students focus  on this week out of class?  [Out-of-Class Labs]', 'out_of_class_labs')\n",
        "rename_col(profta, 'What kinds of activities did your students focus  on this week out of class?  [Multi-Week Project Experience]','multi_week_project')\n",
        "rename_col(profta, 'What kinds of activities did your students focus  on this week out of class?  [Other]','other_outside_class')\n",
        "rename_col(profta, 'By observation, what concepts or processes did students struggle with?','struggle_concepts')\n",
        "rename_col(profta, 'What questions did students raise this week?','questions_raised')\n",
        "rename_col(profta, 'Which student questions were surprising to you?','surprise_questions')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-oyqeeQSp5_",
        "colab_type": "text"
      },
      "source": [
        "# Replace NA values with strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IJk_dULSp6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stud.topics_covered = stud.topics_covered.replace({None:'NA'})\n",
        "stud.reading_research = stud.reading_research.replace({None:'NA'})\n",
        "stud.finishing_class_work = stud.finishing_class_work.replace({None:'NA'})\n",
        "stud.homework = stud.homework.replace({None:'NA'})\n",
        "stud.out_of_class_labs = stud.out_of_class_labs.replace({None:'NA'})\n",
        "stud.multi_week_project = stud.multi_week_project.replace({None:'NA'})\n",
        "stud.other_outside_class = stud.other_outside_class.replace({None:'NA'})\n",
        "stud.struggle_concepts = stud.struggle_concepts.replace({None:'NA'})\n",
        "stud.questions_raised = stud.questions_raised.replace({None:'NA'})\n",
        "stud.surprise_questions = stud.surprise_questions.replace({None:'NA'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul2N7HSYSp6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "profta.topics_covered = profta.topics_covered.replace({None:'NA'})\n",
        "profta.reading_research = profta.reading_research.replace({None:'NA'})\n",
        "profta.finishing_class_work = profta.finishing_class_work.replace({None:'NA'})\n",
        "profta.homework = profta.homework.replace({None:'NA'})\n",
        "profta.out_of_class_labs = profta.out_of_class_labs.replace({None:'NA'})\n",
        "profta.multi_week_project = profta.multi_week_project.replace({None:'NA'})\n",
        "profta.other_outside_class = profta.other_outside_class.replace({None:'NA'})\n",
        "profta.struggle_concepts = profta.struggle_concepts.replace({None:'NA'})\n",
        "profta.questions_raised = profta.questions_raised.replace({None:'NA'})\n",
        "profta.surprise_questions = profta.surprise_questions.replace({None:'NA'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1cydtt3Sp6L",
        "colab_type": "text"
      },
      "source": [
        "# Tokenize function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uly_LEbVSp6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sentence tokenizer for bag of words and n-grams, returns tokenized sentences \n",
        "#for each column\n",
        "def tokenize(csv,col):\n",
        "    whole_text = ''\n",
        "    for sentence in csv[col]:\n",
        "        if not str(sentence).endswith(string.punctuation):\n",
        "            sentence+='. '\n",
        "            whole_text += str(sentence)\n",
        "    tokenized = nltk.sent_tokenize(whole_text)\n",
        "    return tokenized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vws21wy6Sp6Q",
        "colab_type": "text"
      },
      "source": [
        "# Bag of words function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhL6NvexSp6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creates a bag of words for each function\n",
        "def bag_of_words(tokenized):\n",
        "    #extends stop words list with context specific frequent words\n",
        "    stop_words = list(stopwords.words('english'))\n",
        "    ext = ['students','none','na','think','really','nope','cool','new','vs',\n",
        "           'particularly','nothing','week','anything','use','learn','heavily',\n",
        "           'mainly','need','kinds','lots','remember','late','weeks','coming',\n",
        "           'lot','struggled','using','see','hard','student','cant','different',\n",
        "           'questions','surprised','pre','processing','n','2','sick']\n",
        "    stop_words.extend(ext)\n",
        "    #removes non-alphabetical characters and white spaces\n",
        "    for i in range(len(tokenized)):\n",
        "        tokenized[i]=tokenized[i].lower()\n",
        "        tokenized[i] = re.sub(r'\\W',' ',tokenized[i])\n",
        "        tokenized[i] = re.sub(r'\\s+', ' ', tokenized[i])\n",
        "    #removes blank answers\n",
        "    for i in reversed(range(len(tokenized))):\n",
        "        if tokenized[i]==' ':\n",
        "            del tokenized[i]\n",
        "    #creates list of lists, inside lists contains sentences tokenized by word\n",
        "    list_of_lists = []\n",
        "    for sentence in tokenized:\n",
        "        tokens = nltk.word_tokenize(sentence)\n",
        "        sentence_list = []\n",
        "        #removes stopwords\n",
        "        for token in tokens:\n",
        "            if token in stop_words:\n",
        "                continue\n",
        "            else:\n",
        "                sentence_list.append(token)\n",
        "        list_of_lists.append(sentence_list)\n",
        "    return list_of_lists"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q5mCcJlSp6W",
        "colab_type": "text"
      },
      "source": [
        "# Create n-grams function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbm0SQXTSp6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creates list of n-grams\n",
        "def extract_ngrams(col,num):\n",
        "    #creates a list of lists of the sentences tokenized by word\n",
        "    list_of_lists = []\n",
        "    #extends stop words with frequent unneccesary words\n",
        "    stop_words = list(stopwords.words('english'))\n",
        "    ext = ['class','sick','use','heavily','week','need','questions','see',\n",
        "           'late','vs','surprised','lot','wayyy','really','lots','hard',\n",
        "           'student','go','weeks','microsoft']\n",
        "    stop_words.extend(ext)\n",
        "    for sentence in col:\n",
        "      new_sent = ''\n",
        "      for word in sentence.split():\n",
        "        #removes stop words\n",
        "        if word not in stop_words:\n",
        "          new_sent += word\n",
        "          new_sent += ' '\n",
        "      n_grams=ngrams(nltk.word_tokenize(new_sent),num)\n",
        "      ret = [' '.join(grams) for grams in n_grams]\n",
        "      list_of_lists.append(ret)\n",
        "    return list_of_lists"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAUC8Q6SSp6d",
        "colab_type": "text"
      },
      "source": [
        "# Create student bags of words\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "EWpVUmbWSp6e",
        "colab_type": "code",
        "outputId": "1986702b-6bf5-49ec-d9d4-df177df17858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "#Topics Covered\n",
        "stud_topics_tkn = tokenize(stud,'topics_covered')\n",
        "stud_topics_bag = bag_of_words(stud_topics_tkn)\n",
        "\n",
        "stud_topics_ngrams = []\n",
        "ngram_list = extract_ngrams(stud_topics_tkn,2)\n",
        "for ngram in ngram_list:\n",
        "    stud_topics_ngrams.append(ngram)\n",
        "print(\"Student Topics:\",stud_topics_ngrams,'\\n')\n",
        "\n",
        "#Concepts Struggled\n",
        "stud_struggles_tkn = tokenize(stud,'struggle_concepts')\n",
        "stud_struggles_bag = bag_of_words(stud_struggles_tkn)\n",
        "\n",
        "stud_struggles_ngrams = []\n",
        "ngram_list = extract_ngrams(stud_struggles_tkn,2)\n",
        "for ngram in ngram_list:\n",
        "    stud_struggles_ngrams.append(ngram)\n",
        "print(\"Student Struggles:\",stud_struggles_ngrams,'\\n')\n",
        "\n",
        "#Questions Asked\n",
        "stud_questions_tkn = tokenize(stud,'questions_raised')\n",
        "stud_questions_bag = bag_of_words(stud_questions_tkn)\n",
        "\n",
        "stud_questions_ngrams = []\n",
        "ngram_list = extract_ngrams(stud_questions_tkn,2)\n",
        "for ngram in ngram_list:\n",
        "    stud_questions_ngrams.append(ngram)\n",
        "print(\"Student Questions:\",stud_questions_ngrams,'\\n')\n",
        "\n",
        "#Surprise Questions\n",
        "stud_surprise_tkn = tokenize(stud,'surprise_questions')\n",
        "stud_surprise_bag = bag_of_words(stud_surprise_tkn)\n",
        "\n",
        "stud_surprise_ngrams = []\n",
        "ngram_list = extract_ngrams(stud_surprise_tkn,2)\n",
        "for ngram in ngram_list:\n",
        "    stud_surprise_ngrams.append(ngram)\n",
        "print(\"Student Surprise Questions:\",stud_surprise_ngrams,'\\n')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Student Topics: [['datetime pandas', 'pandas python'], ['discussed python', 'python functions', 'functions statistics', 'statistics z', 'z score', 'score iqr', 'iqr variability', 'variability standard', 'standard deviation', 'deviation etc', 'etc worked', 'worked projects', 'projects turned', 'turned first', 'first scaffold', 'scaffold activity'], ['python data', 'data types', 'types arrays', 'arrays briefly'], ['arrays tables', 'tables manipulating', 'manipulating rows', 'rows columns', 'columns types', 'types data', 'data presentations'], ['statistics measures', 'measures central', 'central tendency', 'tendency variability', 'variability eda', 'eda terms'], ['sampling distribution', 'distribution random', 'random samples', 'samples population'], ['tables sorting', 'sorting filtering', 'filtering creating', 'creating data', 'data types', 'types access', 'access different', 'different variables', 'variables within', 'within datatype'], ['standard deviation', 'deviation reviewing', 'reviewing percentiles', 'percentiles mean', 'mean mode', 'mode different', 'different methods', 'methods visualizing', 'visualizing data', 'data histograms', 'histograms box', 'box plots', 'plots bar', 'bar pie', 'pie charts'], ['worked interactive', 'interactive exercise', 'exercise data', 'data processing', 'processing using', 'using data', 'data animal', 'animal shelter'], ['using pandas', 'pandas datetime', 'datetime manipulate', 'manipulate dates', 'dates added', 'added columns', 'columns existing', 'existing table', 'table data', 'data changed', 'changed datatype', 'datatype one', 'one column', 'column data', 'data datetime'], ['learned datetime', 'datetime variables', 'variables work'], ['continuing wrapping', 'wrapping module', 'module manipulating', 'manipulating data', 'data understanding', 'understanding libraries', 'libraries like', 'like pandas', 'pandas numpy'], ['also touched', 'touched briefly', 'briefly arrays', 'arrays worked', 'worked arrays', 'arrays making', 'making new', 'new columns', 'columns old', 'old columns', 'columns dropping', 'dropping selecting', 'selecting columns', 'columns selecting', 'selecting rows', 'rows based', 'based criteria', 'criteria overview', 'overview eda'], ['eda overview', 'overview included', 'included distinguishing', 'distinguishing descriptive', 'descriptive statistics', 'statistics visual', 'visual representations', 'representations data', 'data pros', 'pros cons'], ['covered eda', 'eda topics', 'topics bar', 'bar charts', 'charts histograms', 'histograms boxplots', 'boxplots iqr', 'iqr percentiles', 'percentiles z', 'z scores', 'scores variability', 'variability python', 'python make', 'make visualizations', 'visualizations data', 'data generate', 'generate descriptive', 'descriptive statistics'], ['continued statistics', 'statistics specifically', 'specifically understanding', 'understanding variability', 'variability derived', 'derived identifying', 'identifying extreme', 'extreme values', 'values using', 'using measure', 'measure spread', 'spread calculate', 'calculate simple', 'simple probability', 'probability creating', 'creating cumulative', 'cumulative distribution', 'distribution function', 'function python', 'python connecting', 'connecting histograms', 'histograms empirical', 'empirical rule', 'rule probabilities', 'probabilities distinguishing', 'distinguishing population', 'population sample', 'sample sampling', 'sampling distribution', 'distribution understanding', 'understanding impact', 'impact sample', 'sample size', 'size applying', 'applying central', 'central limit', 'limit theorem', 'theorem normality', 'normality sampling', 'sampling distributions'], ['used salary', 'salary data', 'data practice', 'practice skills'], ['also covered', 'covered holding', 'holding going', 'going forward', 'forward using', 'using zoom'], ['learned trend', 'trend lines', 'lines color', 'color theory', 'theory reviewed', 'reviewed midterm'], ['covered make', 'make box', 'box plot', 'plot visualizations', 'visualizations color', 'color theory', 'theory pick', 'pick colors', 'colors visuals', 'visuals learned', 'learned data', 'data wrangling', 'wrangling filtering'], ['focused data', 'data wrangling', 'wrangling like', 'like summarize', 'summarize mutate', 'mutate arrange'], ['students worked', 'worked projects'], ['maps practice', 'practice exam'], ['reviewed data', 'data wrangling', 'wrangling focus', 'focus tidy', 'tidy data', 'data began', 'began discussing', 'discussing maps'], ['overview five', 'five named', 'named graphs', 'graphs data', 'data visualization', 'visualization introduction', 'introduction color', 'color theory', 'theory well', 'well introduction', 'introduction upcoming', 'upcoming project', 'project data', 'data collection'], ['primarily discussed', 'discussed trend', 'trend lines'], ['many new', 'new topics', 'topics devoted', 'devoted time', 'time review', 'review first', 'first midterm'], ['past review', 'review data', 'data visualization', 'visualization grammar', 'grammar graphics', 'graphics spent', 'spent reviewing', 'reviewing midterm', 'midterm finishing', 'finishing projects'], ['towards end', 'end began', 'began discuss', 'discuss data', 'data wrangling', 'wrangling data', 'data wrangling'], ['went deeper', 'deeper discussing', 'discussing data', 'data wrangling', 'wrangling topics'], ['cover new', 'new topics', 'topics working', 'working project', 'project revolved', 'revolved around', 'around data', 'data wrangling', 'wrangling went', 'went common', 'common coding', 'coding error', 'error messages', 'messages discussed', 'discussed reading', 'reading textbook', 'textbook supports', 'supports learning', 'learning data', 'data science', 'science coding', 'coding worked', 'worked project', 'project mostly', 'mostly creating', 'creating data', 'data graphic'], ['spent one', 'one period', 'period finishing', 'finishing getting', 'getting feedback', 'feedback mini', 'mini projects', 'projects rest', 'rest spent', 'spent learning', 'learning functions', 'functions r', 'r manipulate', 'manipulate data'], ['learned five', 'five main', 'main verbs', 'verbs r', 'r package', 'package dplyr', 'dplyr went', 'went depth', 'depth pipe', 'pipe function', 'function talked', 'talked coding', 'coding style', 'style etc'], ['joining tables', 'tables r', 'r concept', 'concept tidy', 'tidy data', 'data writing', 'writing functions', 'functions r', 'r iteration', 'iteration within', 'within writing', 'writing functions', 'functions r', 'r map', 'map function', 'function github', 'github upcoming', 'upcoming group', 'group project'], ['looked data', 'data set', 'set using', 'using current', 'current mini', 'mini project', 'project import', 'import data', 'data r', 'r outside', 'outside sources'], ['also discussed', 'discussed going', 'going look', 'look transition', 'transition remote', 'remote learning', 'learning talked', 'talked little', 'little projects', 'projects r', 'r worked', 'worked ggplots2', 'ggplots2 mostly', 'mostly worked', 'worked small', 'small groups', 'groups mini', 'mini projects', 'projects talked', 'talked dplyr', 'dplyr package', 'package r', 'r encompasses', 'encompasses 5', '5 main', 'main functions'], ['explained like', 'like ggplot2', 'ggplot2 wherein', 'wherein grammar', 'grammar code', 'code dplyr', 'dplyr grammar', 'grammar 5', '5 functions'], ['basically went', 'went functions', 'functions practiced', 'practiced using', 'using labs', 'labs able', 'able work', 'work data', 'data wrangling', 'wrangling github'], ['5 graphs', 'graphs histogram', 'histogram barplot', 'barplot boxplot', 'boxplot scatterplot', 'scatterplot linearplot', 'linearplot color', 'color theory'], ['trend lines', 'lines midterm', 'midterm mini', 'mini project'], ['worked mini', 'mini project', 'project 2'], [], ['five named', 'named graphs', 'graphs barplots', 'barplots boxplots', 'boxplots intro', 'intro color', 'color theory'], ['color palettes', 'palettes loess', 'loess tool', 'tool shiny', 'shiny app'], ['data wrangling'], ['data wrangling', 'wrangling tidy', 'tidy data', 'data tables'], ['worked little', 'little tidy', 'tidy data', 'data mostly', 'mostly finishing', 'finishing group', 'group projects'], ['midterm review', 'review started', 'started covering', 'covering maps', 'maps gis', 'gis barely'], ['understanding addressing', 'addressing errors', 'errors r', 'r making', 'making data', 'data visuals', 'visuals bit', 'bit data', 'data wrangling', 'wrangling sing', 'sing table', 'table looked', 'looked five', 'five basic', 'basic verbs', 'verbs data', 'data wrangling', 'wrangling tiidy', 'tiidy data'], ['data wrangling', 'wrangling github', 'github iteration', 'iteration mapping'], ['random forest', 'forest classification', 'classification tree', 'tree probability', 'probability tree'], ['gradient descent', 'descent neural', 'neural networks'], ['dbscan k', 'k means', 'means clustering', 'clustering general', 'general clustering', 'clustering algorithms'], ['neural networks', 'networks applications', 'applications tensorflow'], ['natural language', 'language processing'], ['data types'], ['data integration', 'integration entity', 'entity resolution'], ['mostly review', 'review exam'], ['naive bayes']] \n",
            "\n",
            "Student Struggles: [[], ['struggled bit', 'bit statistics', 'statistics much', 'much background', 'background pretty', 'pretty easy', 'easy catch', 'catch understanding', 'understanding individual', 'individual line', 'line code', 'code meant', 'meant python', 'python purpose', 'purpose word', 'word comfort', 'comfort google', 'google colab'], ['tables continuing', 'continuing preprocess', 'preprocess data'], ['understanding random', 'random sampling', 'sampling distributions', 'distributions covered', 'covered better', 'better understand', 'understand make', 'make sense', 'sense variation', 'variation distribution'], ['sampling distribution', 'distribution means', 'means distribution'], ['idea sampling', 'sampling distribution', 'distribution super', 'super clear'], ['figuring navigate', 'navigate minimal', 'minimal syntax', 'syntax differences', 'differences google', 'google colab', 'colab pycharm', 'pycharm minor', 'minor bugs', 'bugs initial', 'initial coding', 'coding libraries', 'libraries defined', 'defined incorrectly', 'incorrectly etc'], ['standard deviation'], ['struggled bit', 'bit importing', 'importing data', 'data co', 'co lab', 'lab mounting', 'mounting data'], ['struggled understanding', 'understanding syntax', 'syntax changing', 'changing datatype', 'datatype data'], ['arrays lists', 'lists syntax', 'syntax around', 'around splitting', 'splitting string', 'string creating', 'creating new', 'new columns', 'columns existing', 'existing one'], ['struggled understanding', 'understanding percentiles', 'percentiles read', 'read boxplot'], ['still struggling', 'struggling importing', 'importing csv', 'csv drive', 'drive co', 'co lab', 'lab file'], ['struggling concepts', 'concepts distribution', 'distribution variability', 'variability histograms', 'histograms fit', 'fit picture', 'picture able', 'able get', 'get shiny', 'shiny app', 'app working', 'working due', 'due technical', 'technical problems', 'problems r', 'r downloads', 'downloads understand', 'understand trend', 'trend lines', 'lines well', 'well would', 'would want', 'want homework', 'homework assignment', 'assignment time', 'time choosing', 'choosing type', 'type graph', 'graph would', 'would appropriate', 'appropriate idea', 'idea wanted', 'wanted present'], ['confused would', 'would filter', 'filter remove', 'remove condition', 'condition would', 'would still', 'still bc', 'bc already', 'already changed', 'changed data', 'data run', 'run confusing', 'confusing first'], [], ['codebook house', 'house elections', 'elections data', 'data sucks'], ['understand variables', 'variables meant', 'meant find', 'find map', 'map data', 'data peers', 'peers trouble', 'trouble understanding', 'understanding makes', 'makes data', 'data tidy'], ['struggle anything', 'anything particular', 'particular know', 'know peers', 'peers still', 'still struggling', 'struggling using', 'using knit', 'knit feature', 'feature r', 'r markdown'], ['think anyone', 'anyone particularly', 'particularly struggled', 'struggled topics', 'topics quite', 'quite simple', 'simple understanding', 'understanding data', 'data wrangling', 'wrangling difficult', 'difficult part', 'part peers', 'peers struggled', 'struggled using', 'using filter', 'filter function', 'function dplyr', 'dplyr package', 'package students', 'students understood', 'understood end'], ['many people', 'people struggled', 'struggled running', 'running data', 'data wrangling', 'wrangling code', 'code proper', 'proper order', 'order students', 'students understood', 'understood material', 'material needed', 'needed help', 'help actually', 'actually applying', 'applying concepts'], ['joining data', 'data frames'], ['students working', 'working spent', 'spent time', 'time figuring', 'figuring translate', 'translate idea', 'idea graph', 'graph heads', 'heads actual', 'actual code', 'code would', 'would create', 'create graph', 'graph still', 'still thinking', 'thinking makes', 'makes graphics', 'graphics compelling', 'compelling comprehensive'], ['work lab', 'lab pretty', 'pretty straight', 'straight forward', 'forward part', 'part peers', 'peers struggling', 'struggling much', 'much would', 'would say', 'say anything', 'anything getting', 'getting hang', 'hang different', 'different functions', 'functions r', 'r gave', 'gave us', 'us trouble'], ['remembering mean', 'mean choose', 'choose one', 'one professor', 'professor baumer', 'baumer asked', 'asked us', 'us trouble', 'trouble went', 'went functions', 'functions depth', 'depth getting', 'getting hang', 'hang different', 'different kinds', 'kinds join', 'join functions', 'functions pivot', 'pivot functions', 'functions within', 'within tidy', 'tidy data'], ['people including', 'including also', 'also pretty', 'pretty confused', 'confused dealing', 'dealing missing', 'missing data', 'data joining', 'joining tables', 'tables ran', 'ran technical', 'technical issues', 'issues getting', 'getting github', 'github going'], ['setting github', 'github getting', 'getting access', 'access groups', 'groups repositories', 'repositories cloning', 'cloning repositories', 'repositories r', 'r getting', 'getting used', 'used github', 'github pushing', 'pushing pulling', 'pulling small', 'small errors', 'errors happening', 'happening professor', 'professor baumer', 'baumer made', 'made smooth', 'smooth comprehensible', 'comprehensible transition', 'transition could'], ['biggest part', 'part course', 'course talking', 'talking difficulties', 'difficulties arise', 'arise us', 'us moving', 'moving campus', 'campus able', 'able meet'], ['one specific', 'specific struggle', 'struggle encountered', 'encountered opinion', 'opinion talked', 'talked kind', 'kind problems', 'problems face'], ['personally issues', 'issues figuring', 'figuring certain', 'certain aspects', 'aspects ggplot', 'ggplot specifically', 'specifically ggrepel', 'ggrepel function', 'function introduced', 'introduced homework'], ['also still', 'still struggle', 'struggle logic', 'logic coding', 'coding something', 'something issues', 'issues past', 'past coding', 'coding classes'], ['would say', 'say learning', 'learning functions', 'functions garnered', 'garnered many', 'many us', 'us unfamiliar', 'unfamiliar needed', 'needed time', 'time exposure', 'exposure struggled', 'struggled git', 'git push', 'push pull', 'pull commit', 'commit ideas', 'ideas setting', 'setting general'], ['struggled boxplots', 'boxplots shape', 'shape center', 'center spread'], ['struggled understanding', 'understanding loess', 'loess function', 'function well', 'well make', 'make tidy', 'tidy app'], ['formatting issues', 'issues project'], ['tidy data'], ['figuring add', 'add custom', 'custom colors', 'colors different', 'different kinds', 'kinds graphs'], ['new material', 'material midterm'], ['summary statistics'], ['tidy data', 'data table', 'table effectively'], ['anything new'], ['getting projects', 'projects done'], ['tidy data', 'data midterm', 'midterm review', 'review understanding', 'understanding metadata', 'metadata context', 'context data', 'data frames'], ['group trouble', 'trouble choosing', 'choosing map', 'map variables', 'variables onto', 'onto data', 'data graphic', 'graphic effectively', 'effectively without', 'without simple'], ['struggling make', 'make data', 'data graphic', 'graphic good', 'good still', 'still meeting', 'meeting grade', 'grade requirements'], ['also struggle', 'struggle downloading', 'downloading installing', 'installing things', 'things general'], ['kinda scary', 'scary know', 'know exactly', 'exactly im', 'im putting', 'putting laptop', 'laptop hitting', 'hitting next', 'next everything'], ['summarize function', 'function bit', 'bit grasp', 'grasp well', 'well piping', 'piping functions', 'functions efficient', 'efficient manner'], ['struggled utilizing', 'utilizing five', 'five verbs', 'verbs well', 'well whatever', 'whatever new', 'new function', 'function learning', 'learning simultaneously'], ['could understand', 'understand quite', 'quite clearly', 'clearly inability', 'inability successfully', 'successfully utilize', 'utilize five', 'five verbs', 'verbs reach', 'reach result', 'result wanted', 'wanted new', 'new function', 'function e', 'e making', 'making function', 'function made', 'made everything', 'everything difficult', 'difficult frustrating'], ['peers still', 'still confused', 'confused filter', 'filter function', 'function well', 'well import', 'import data'], ['parse_number function', 'function also', 'also unclear'], ['return function', 'function also', 'also still', 'still unclear'], ['also confusion', 'confusion n', 'n babynames', 'babynames package', 'package also', 'also n'], [], ['noticed grasp', 'grasp five', 'five verbs', 'verbs great', 'great thought', 'thought start', 'start creating', 'creating functions', 'functions could', 'could get', 'get functions', 'functions wanted'], ['probably hardest', 'hardest semester'], ['got worse', 'worse figuring', 'figuring map', 'map point', 'point feel', 'feel like', 'like blanket', 'blanket fixes', 'fixes code'], ['code solutions', 'solutions different', 'different ways', 'ways sometimes', 'sometimes specific', 'specific fixes', 'fixes kinda', 'kinda get', 'get specific', 'specific help', 'help large', 'large job', 'job prevents', 'prevents going', 'going office', 'office hours'], ['able get', 'get help', 'help professor', 'professor though', 'though many', 'many others', 'others seemingly', 'seemingly needed', 'needed help', 'help unsure', 'unsure asking', 'asking getting'], ['specific mapping', 'mapping iteration', 'iteration functions', 'functions previous'], ['putting everything', 'everything together', 'together rough', 'rough homework', 'homework 4', '4 babynames', 'babynames copy', 'copy master', 'master seems', 'seems ruffling', 'ruffling everyone'], ['find hw4', 'hw4 okay', 'okay consensus', 'consensus spoke', 'spoke tutoring', 'tutoring hours', 'hours rough', 'rough manipulating', 'manipulating data', 'data types', 'types unique', 'unique data', 'data sets', 'sets algorithms'], ['morphing changing', 'changing structure', 'structure nn', 'nn fit', 'fit unique', 'unique data', 'data sets', 'sets normalizing', 'normalizing data', 'data took', 'took effort'], ['reading data', 'data manipulating', 'manipulating data', 'data varied', 'varied tasks'], ['package instillation', 'instillation data', 'data manipulation'], ['times bit', 'bit difficult', 'difficult distinguish', 'distinguish ratio', 'ratio interval', 'interval date', 'date types'], ['struggled technology', 'technology github', 'github python', 'python think', 'think figured', 'figured know'], ['struggled reviewing', 'reviewing exam'], ['although review', 'review sheet', 'sheet multiple', 'multiple choice', 'choice question', 'question data', 'data camp', 'camp bit', 'bit confusing'], ['always think', 'think difficult', 'difficult think', 'think programs', 'programs physical', 'physical sheet', 'sheet paper', 'paper laptop', 'laptop group', 'group struggled', 'struggled come', 'come best', 'best algorithms', 'algorithms run', 'run data', 'data clean'], ['needed determine', 'determine best', 'best way', 'way reshape', 'reshape dataset']] \n",
            "\n",
            "Student Questions: [['load data', 'data github', 'github python'], ['clarify homework', 'homework assignment'], ['also asked', 'asked worked', 'worked activities'], ['asking differences', 'differences arrays', 'arrays lists', 'lists data', 'data types'], ['syntax building', 'building functions', 'functions manipulating', 'manipulating arrays'], ['boxplots percentiles', 'percentiles iqr'], ['difficulties running', 'running colab'], ['better alternative', 'alternative colab', 'colab define', 'define one', 'one level', 'level expertise', 'expertise python', 'python mostly', 'mostly issues', 'issues surrounding', 'surrounding accessing', 'accessing getting', 'getting comfortable', 'comfortable coding', 'coding colab', 'colab well', 'well familiarizing', 'familiarizing python'], ['calculate iqr'], ['standard deviation'], ['z scores'], ['asking review', 'review difference', 'difference eda', 'eda processing', 'processing data', 'data well', 'well sorting'], ['one peer', 'peer asked', 'asked advanced', 'advanced text', 'text manipulation', 'manipulation techniques', 'techniques python'], ['asked create', 'create frequency', 'frequency distributions', 'distributions reviewing', 'reviewing difference', 'difference arrays', 'arrays lists', 'lists locating', 'locating pandas', 'pandas numpy', 'numpy documentation'], ['peers raised', 'raised statistical', 'statistical concepts', 'concepts python', 'python asked', 'asked coding', 'coding opportunities', 'opportunities clarification', 'clarification read', 'read code', 'code finding', 'finding standard', 'standard deviation', 'deviation dataset'], [], ['asked trend', 'trend lines', 'lines didnt', 'didnt understand', 'understand different', 'different variation'], ['asked difference', 'difference color', 'color pallette', 'pallette filters', 'filters would', 'would photocopy', 'photocopy friendly', 'friendly none'], [], ['constitutes non', 'non redundant', 'redundant graph'], ['coordinate reference', 'reference system', 'system projects', 'projects work'], ['data wrangling', 'wrangling general', 'general focus', 'focus next', 'next midterm'], ['many logistical', 'logistical related', 'related upcoming', 'upcoming project', 'project appropriate', 'appropriate data', 'data variables', 'variables consider', 'consider best', 'best way', 'way collect', 'collect record', 'record major', 'major primarily', 'primarily revolved', 'revolved around', 'around logistics', 'logistics definition', 'definition loess', 'loess smoother'], ['general students', 'students specific', 'specific coding', 'coding forgotten', 'forgotten write', 'write code', 'code something', 'something add', 'add labels'], ['question error', 'error message', 'message continued', 'continued appear', 'appear r', 'r console', 'console professor', 'professor explained', 'explained r', 'r markdown', 'markdown knit', 'knit html', 'html viewer', 'viewer part', 'part code'], ['properly pipe', 'pipe operator'], ['nothing particular', 'particular comes', 'comes mind', 'mind related', 'related data', 'data wrangling', 'wrangling order', 'order complete', 'complete project', 'project campaign', 'campaign contributions'], ['group project', 'project mostly', 'mostly manipulating', 'manipulating graphs', 'graphs like', 'like rearrange', 'rearrange bars', 'bars bar', 'bar graph', 'graph manipulate', 'manipulate color', 'color palettes', 'palettes choose', 'choose right', 'right one', 'one adjusting', 'adjusting legend', 'legend key', 'key graphic', 'graphic pipe', 'pipe function', 'function different', 'different saying'], ['function head', 'head difference', 'difference tbl', 'tbl data', 'data frame'], ['specific order', 'order different', 'different functions', 'functions coding'], ['discussion appropriate', 'appropriate kind', 'kind join', 'join function', 'function kind', 'kind scenario', 'scenario would', 'would necessary'], ['asked technical', 'technical issues', 'issues github'], ['peers asked', 'asked importing', 'importing data', 'data outside', 'outside sources', 'sources directly', 'directly addressed', 'addressed professor', 'professor baumer', 'baumer lecture', 'lecture asked', 'asked stacked', 'stacked bar', 'bar plots', 'plots wrangling', 'wrangling data', 'data error', 'error messages'], ['differences uses', 'uses summarize'], ['slack new', 'new data', 'data set', 'set using', 'using next', 'next mini', 'mini project'], ['mainly github'], ['mainly housekeeping', 'housekeeping related', 'related understanding', 'understanding formatting', 'formatting r', 'r studio'], ['mainly formatting', 'formatting coding', 'coding first', 'first mini', 'mini project', 'project data', 'data visualization', 'visualization explanations', 'explanations midterm'], ['r markdown'], [], ['decide kinds', 'kinds graphs', 'graphs different', 'different kinds'], [], [], [], ['project related'], [], ['mostly group', 'group work', 'work hear', 'hear many', 'many peer'], ['group package', 'package installation', 'installation related', 'related well', 'well aesthetic', 'aesthetic mapping'], ['asked summarize'], ['peers asked', 'asked bit', 'bit data', 'data frame', 'frame tbl', 'tbl differences'], ['functions used', 'used lab', 'lab kind', 'kind confusing', 'confusing asked', 'asked turned', 'turned explained', 'explained book', 'book chapter'], ['data wrangling', 'wrangling lab', 'lab exercises', 'exercises depended', 'depended person', 'person based', 'based successfully', 'successfully connect', 'connect four', 'four verbs'], ['mainly function', 'function specific', 'specific asking', 'asking clarification', 'clarification data', 'data went', 'went pivot_long', 'pivot_long pivit_wider', 'pivit_wider join', 'join functions', 'functions super', 'super specific', 'specific especially', 'especially came', 'came using', 'using github'], ['mapping grouped', 'grouped data', 'data using', 'using map', 'map along', 'along functions'], ['manipulating data', 'data types', 'types unique', 'unique data', 'data sets', 'sets algorithms'], [], [], [], [], ['asked interval', 'interval meant', 'meant inout', 'inout capped', 'capped certain'], ['told case'], ['made sure', 'sure using', 'using repositories', 'repositories correctly', 'correctly saving', 'saving data', 'data correct', 'correct files'], ['several take', 'take home', 'home midterm', 'midterm format', 'format would', 'would best', 'best turn', 'turn properly', 'properly open', 'open dataset', 'dataset files', 'files python']] \n",
            "\n",
            "Student Surprise Questions: [[], [], ['good way', 'way raised', 'raised easily', 'easily google', 'google able', 'able answerable', 'answerable spent', 'spent time', 'time looking', 'looking options', 'options tabs', 'tabs within', 'within google', 'google colab', 'colab e'], ['shortcut run', 'run specific', 'specific line', 'line code'], [], [], [], ['arrays list'], ['still curious', 'curious similar', 'similar different', 'different would', 'would love'], ['people fully', 'fully familiar', 'familiar calculating', 'calculating percentiles', 'percentiles working', 'working bell', 'bell curves', 'curves z', 'z scores'], ['question advanced', 'advanced text', 'text manipulation', 'manipulation techniques', 'techniques python', 'python introductory', 'introductory course', 'course personally', 'personally thinking', 'thinking advanced', 'advanced techniques'], ['however professor', 'professor clark', 'clark brought', 'brought two', 'two cool', 'cool text', 'text manipulation', 'manipulation based', 'based projects', 'projects knows', 'knows going', 'going right', 'right cool', 'cool learn'], ['one asking', 'asking define', 'define function', 'function already', 'already premade', 'premade us', 'us pandas', 'pandas also', 'also curious'], [], [], [], [], [], [], [], [], [], [], ['somewhat variation', 'variation statistical', 'statistical background', 'background knowledge', 'knowledge standard', 'standard deviation', 'deviation linear', 'linear regression', 'regression trend', 'trend lines', 'lines based', 'based expected', 'expected particularly'], [], [], [], [], ['maybe surprising', 'surprising right', 'right word', 'word one', 'one asked', 'asked exactly', 'exactly data', 'data frame', 'frame tibble', 'tibble changed', 'changed perform', 'perform different', 'different functions', 'functions like', 'like say', 'say mutating', 'mutating column'], ['think understood', 'understood beforehand', 'beforehand changes', 'changes environment', 'environment change', 'change underlying', 'underlying data', 'data r', 'r talking', 'talking got', 'got thinking', 'thinking interact', 'interact data', 'data reminded', 'reminded careful', 'careful remembering', 'remembering data', 'data part', 'part initial', 'initial frame', 'frame data', 'data created', 'created result', 'result manipulation'], ['someone asked', 'asked join', 'join data', 'data frames', 'frames vertically', 'vertically thought', 'thought interested', 'interested learn', 'learn happen', 'happen would', 'would na'], [], ['somebody brought', 'brought google', 'google colab', 'colab never', 'never heard', 'heard also', 'also mentioned', 'mentioned concepts', 'concepts idea', 'idea knew', 'knew particularly'], ['new material'], [], ['certain codes', 'codes seemed', 'seemed tedious', 'tedious midterm', 'midterm answers', 'answers thought', 'thought difficult'], [], [], [], [], [], [], [], [], [], [], ['cant remember'], ['people coding', 'coding background', 'background made', 'made sense', 'sense asked', 'asked cant', 'cant recall', 'recall enough', 'enough type', 'type na'], [], [], [], [], [], ['peers seemed', 'seemed struggle', 'struggle distinguishing', 'distinguishing quantitative', 'quantitative data', 'data qualitative', 'qualitative data'], [], ['people asked', 'asked think', 'think surprising', 'surprising since', 'since right', 'right exam', 'exam types', 'types expected'], []] \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gha0rKeqt8B8",
        "colab_type": "text"
      },
      "source": [
        "# Create Prof./TA Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRW_VGfaSp6k",
        "colab_type": "code",
        "outputId": "a4cf911d-112f-473c-91ad-ffca3457db90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "#Topics Covered\n",
        "profta_topics_tkn = tokenize(profta,'topics_covered')\n",
        "profta_topics_bag = bag_of_words(profta_topics_tkn)\n",
        "\n",
        "profta_topics_ngrams = []\n",
        "ngram_list = extract_ngrams(profta_topics_tkn,2)\n",
        "for ngram in ngram_list:\n",
        "    profta_topics_ngrams.append(ngram)\n",
        "print(\"Professor/TA Topics:\",profta_topics_ngrams,'\\n')\n",
        "\n",
        "#Concepts Struggled\n",
        "profta_struggles_tkn = tokenize(profta,'struggle_concepts')\n",
        "profta_struggles_bag = bag_of_words(profta_struggles_tkn)\n",
        "\n",
        "profta_struggles_ngrams = []\n",
        "ngram_list = extract_ngrams(profta_struggles_tkn,2)\n",
        "for ngram in ngram_list:\n",
        "    profta_struggles_ngrams.append(ngram)\n",
        "print(\"Professor/TA Struggles:\",profta_struggles_ngrams,'\\n')\n",
        "\n",
        "#Questions Asked\n",
        "profta_questions_tkn = tokenize(profta,'questions_raised')\n",
        "profta_questions_bag = bag_of_words(profta_questions_tkn)\n",
        "\n",
        "profta_questions_ngrams = []\n",
        "ngram_list = extract_ngrams(profta_questions_tkn,2)\n",
        "for ngram in ngram_list:\n",
        "    profta_questions_ngrams.append(ngram)\n",
        "print(\"Professor/TA Questions:\",profta_questions_ngrams,'\\n')\n",
        "\n",
        "#Surprise Questions\n",
        "profta_surprise_tkn = tokenize(profta,'surprise_questions')\n",
        "profta_surprise_bag = bag_of_words(profta_surprise_tkn)\n",
        "\n",
        "profta_surprise_ngrams = []\n",
        "ngram_list = extract_ngrams(profta_surprise_tkn,2)\n",
        "for ngram in ngram_list:\n",
        "    profta_surprise_ngrams.append(ngram)\n",
        "print(\"Professor/TA Surprise Questions:\",profta_surprise_ngrams,'\\n')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Professor/TA Topics: [['meeting scheduling', 'scheduling project', 'project group', 'group introductions', 'introductions organization'], ['github idear', 'idear exploratory', 'exploratory data', 'data analysis', 'analysis data', 'data acquisition'], ['data acquisition', 'acquisition exploration', 'exploration github', 'github setups'], ['data acquisition', 'acquisition pre', 'pre processing'], ['barplots color', 'color theory', 'theory data', 'data collection'], ['midterm review', 'review data', 'data viz', 'viz topics', 'topics trend', 'trend lines', 'lines regression', 'regression loess', 'loess shiny', 'shiny interactive', 'interactive visualizations', 'visualizations rstudio', 'rstudio project'], ['intro data', 'data wrangling', 'wrangling working', 'working projects'], ['data wrangling', 'wrangling grouping', 'grouping rows', 'rows summarizing', 'summarizing rows', 'rows creating', 'creating new', 'new variables', 'variables sorting', 'sorting rows', 'rows dataset'], ['joining merging', 'merging data', 'data frames', 'frames data', 'data wrangling', 'wrangling exercises'], ['students worked', 'worked projects'], ['maps practice', 'practice exam'], ['maps gis', 'gis api'], ['guy github', 'github website', 'website deployment'], [], ['cover much', 'much new', 'new material', 'material since', 'since using', 'using fair', 'fair amount', 'amount time', 'time work', 'work projects'], ['clear number', 'number misconceptions', 'misconceptions coding', 'coding mistakes', 'mistakes surrounding', 'surrounding ggplot2', 'ggplot2 well', 'well discussed', 'discussed workflow', 'workflow issues', 'issues like', 'like rstudio', 'rstudio projects', 'projects github'], ['dplyr single', 'single table', 'table verbs', 'verbs aggregation'], ['joining tables', 'tables tidying', 'tidying data', 'data writing', 'writing functions'], ['iteration github'], ['data set', 'set working', 'working importing', 'importing data', 'data r', 'r python', 'python programming', 'programming jan', 'jan 13th'], ['data types', 'types project', 'project management'], ['github project', 'project management', 'management data', 'data preprocessing', 'preprocessing data', 'data cleaning', 'cleaning exploratory', 'exploratory data', 'data analysis'], ['intro machine', 'machine learning', 'learning naive', 'naive bayes'], ['pre processing', 'processing data'], ['working tables', 'tables python', 'python exploratory', 'exploratory data', 'data analysis'], ['exploratory data', 'data analysis', 'analysis variance', 'variance random', 'random sample', 'sample generation'], ['scheduling meetings', 'meetings project', 'project organization'], ['github repo', 'repo creation', 'creation project', 'project planning'], [], ['machine learning', 'learning layered', 'layered design', 'design schedule'], ['scheduling meetings', 'meetings client'], ['setting github', 'github repository', 'repository raw', 'raw data', 'data exploration', 'exploration meeting', 'meeting clients'], ['project milestone', 'milestone 1', '1 view', 'view project', 'project goals', 'goals expected', 'expected outputs'], ['data preprocessing'], ['milestone 3', '3 project', 'project planning', 'planning phase'], ['supervised learning', 'learning unsupervised', 'unsupervised learning']] \n",
            "\n",
            "Professor/TA Struggles: [[], ['grasping github'], ['even explaining', 'explaining purpose', 'purpose process', 'process github', 'github still', 'still challenging', 'challenging students', 'students understand', 'understand get', 'get board'], ['probably data', 'data cleaning', 'cleaning briefly', 'briefly like', 'like good', 'good practices', 'practices order', 'order change', 'change nulls', 'nulls zeros', 'zeros changing', 'changing excel', 'excel github', 'github getting', 'getting better'], ['simple coding', 'coding creating', 'creating layered', 'layered designs', 'designs turn'], ['understanding data', 'data collecting', 'collecting first', 'first mini', 'mini project'], [], ['early introduce'], ['mostly b', 'b c', 'c debugging', 'debugging shiny', 'shiny pita', 'pita many', 'many students', 'students experience', 'experience debugging', 'debugging shiny', 'shiny much', 'much struggle', 'struggle wanted', 'wanted data', 'data wrangling', 'wrangling covered', 'covered yet'], ['data wrangling'], ['students struggled', 'struggled problem', 'problem set', 'set including', 'including debugging', 'debugging r', 'r markdown', 'markdown documents'], ['reaching point', 'point students', 'students learn', 'learn learn', 'learn debug'], ['difference types', 'types joins', 'joins left_join', 'left_join inner_join'], ['constitutes non', 'non redundant', 'redundant graph'], ['coordinate reference', 'reference system', 'system projects', 'projects work'], ['learning decipher', 'decipher error', 'error messages', 'messages r', 'r learning', 'learning new', 'new tools', 'tools na'], ['github terminology'], ['students still', 'still struggling', 'struggling understand', 'understand difference', 'difference mapping', 'mapping aesthetic', 'aesthetic e'], ['color variable', 'variable setting', 'setting aesthetic', 'aesthetic constant', 'constant value'], ['students learning', 'learning manipulate', 'manipulate scales', 'scales especially', 'especially color', 'color previously', 'previously expressed', 'expressed confusion', 'confusion scales'], ['many students', 'students asked', 'asked data', 'data cleaning', 'cleaning data', 'data wrangling', 'wrangling good', 'good cover', 'cover next'], [], ['wrap minds', 'minds around', 'around idea', 'idea collapsing', 'collapsing data', 'data frame', 'frame one', 'one row', 'row using', 'using aggregate', 'aggregate functions'], ['seem ok', 'ok tell', 'tell much', 'much trouble', 'trouble next', 'next two', 'two asked', 'asked put', 'put concepts', 'concepts practice', 'practice much', 'much less', 'less scaffolding'], ['many students', 'students still', 'still time', 'time wrangling', 'wrangling data', 'data given', 'given open', 'open ended', 'ended problem'], ['many students', 'students struggling', 'struggling understand', 'understand functions', 'functions work', 'work map', 'map works'], ['file paths', 'paths remain', 'remain mystery', 'mystery students'], ['understanding file', 'file system', 'system quite', 'quite limited'], ['ordering operations', 'operations git', 'git debugging', 'debugging errors', 'errors python', 'python code'], ['struggled differences', 'differences interval', 'interval ratio', 'ratio data', 'data types'], ['struggled ambiguity', 'ambiguity inherent', 'inherent data', 'data classification', 'classification insufficient', 'insufficient information', 'information given', 'given data'], ['setting github', 'github repositories', 'repositories proved', 'proved extremely', 'extremely challenging'], ['able assess', 'assess picked', 'picked data', 'data cleaning', 'cleaning mid', 'mid feb', 'feb start', 'start using', 'using skills'], ['grasping naive', 'naive bayes'], ['grasping idea', 'idea iterative', 'iterative project', 'project planning'], ['applying foundational', 'foundational coding', 'coding skills', 'skills learned', 'learned prerequisite', 'prerequisite cs', 'cs actual', 'actual dataframe', 'dataframe manipulation'], ['variety programming', 'programming options', 'options available', 'available within', 'within python'], ['document thought', 'thought process', 'process around', 'around going', 'going projects'], ['using github', 'github tool', 'tool project', 'project management'], ['navigating commandline'], ['dataset structuring'], ['naive bayes'], ['project management', 'management collaboration'], ['excel operations', 'operations check', 'check data', 'data understanding', 'understanding data'], ['presentation skills', 'skills prepared'], ['data cleaning', 'cleaning reading', 'reading file', 'file replacing', 'replacing creating', 'creating bins', 'bins data', 'data python'], [], ['understanding applying', 'applying right', 'right alogirthm', 'alogirthm fits', 'fits data']] \n",
            "\n",
            "Professor/TA Questions: [[], ['github github'], [], ['concerns midterm', 'midterm take', 'take home', 'home exam', 'exam tackle', 'tackle data'], ['manually change', 'change colors', 'colors bars', 'bars barplot', 'barplot using', 'using ggplot2'], ['deep question', 'question specifics', 'specifics loess', 'loess smoother', 'smoother algorithm', 'algorithm mostly', 'mostly clarifications', 'clarifications first', 'first project', 'project expectations'], ['first time', 'time assigned', 'assigned sufficient', 'sufficient minimally', 'minimally viable', 'viable product'], ['definitely clean', 'clean things', 'things next', 'next time', 'time assign', 'assign r', 'r markdown', 'markdown document', 'document compiling', 'compiling na'], ['codebook house', 'house elections', 'elections data', 'data sucks'], ['understand variables', 'variables meant', 'meant find', 'find map', 'map data', 'data navigate', 'navigate error', 'error messages'], ['deploy local', 'local version', 'version webpage', 'webpage online'], ['hell github'], ['would evaluated', 'evaluated first', 'first mini', 'mini project', 'project could', 'could collaborate', 'collaborate effectively', 'effectively group', 'group project', 'project clean', 'clean data', 'data reshape', 'reshape data'], ['whether matters', 'matters order', 'order put', 'put pipeline', 'pipeline na', 'na values'], ['deal missing', 'missing values', 'values pass', 'pass values', 'values function', 'function order', 'order write', 'write operations', 'operations pipeline'], ['figure ggplot2', 'ggplot2 like', 'like change', 'change font'], ['still widespread', 'widespread confusion', 'confusion github'], ['campaign finance', 'finance data', 'data set', 'set working', 'working second', 'second mini', 'mini project'], ['issues recognizing', 'recognizing differences', 'differences lists', 'lists arrays', 'arrays issues', 'issues missing', 'missing closing', 'closing symbols', 'symbols previous', 'previous lines', 'lines none', 'none recall', 'recall last'], ['using github'], ['connect learning', 'learning projects', 'projects lectures'], ['handle noisy', 'noisy data', 'data logistics'], ['concerns import', 'import data', 'data jupyter'], ['differences jupyter', 'jupyter notebooks', 'notebooks python', 'python scripts', 'scripts recognize', 'recognize underlying', 'underlying coding', 'coding python', 'python libraries'], ['mainly python', 'python programming'], ['multiple methods', 'methods execute', 'execute outcomes'], ['information percentiles', 'percentiles opportunities', 'opportunities coding'], ['project management', 'management github', 'github usage'], ['create github', 'github repos'], ['topic model'], ['github project', 'project structure', 'structure ordering'], ['regarding programming', 'programming knowledge', 'knowledge work', 'work project'], ['look important', 'important columns', 'columns columns', 'columns necessary', 'necessary understanding', 'understanding requirement'], ['clients insights', 'insights output', 'output project'], ['approach cleaning', 'cleaning data', 'data managing', 'managing work', 'work midterms'], ['programming python'], ['regarding alogrithms', 'alogrithms needs', 'needs used', 'used data']] \n",
            "\n",
            "Professor/TA Surprise Questions: [[], ['github basically', 'basically us', 'us look', 'look peoples', 'peoples old', 'old projects'], ['people editing', 'editing raw', 'raw data', 'data students', 'students talked', 'talked struggled', 'struggled midterm'], ['significant amount', 'amount little', 'little coding', 'coding knowledge', 'knowledge struggled', 'struggled even', 'even basic', 'basic commands'], ['nothing stands'], [], ['none remember'], [], [], ['completed survey'], ['completed survey'], [], [], [], ['many students', 'students eager', 'eager github', 'github even', 'even though', 'though told', 'told works', 'works yet'], ['approach let', 'let feel', 'feel pain', 'pain collaborating', 'collaborating without', 'without github', 'github first', 'first pain', 'pain motivation', 'motivation learn', 'learn github', 'github next', 'next time'], ['wondering rigorous', 'rigorous github', 'github proselytizing', 'proselytizing starting', 'starting pay', 'pay dividends'], ['many students', 'students struggled', 'struggled compute', 'compute bmi', 'bmi correctly', 'correctly hw3', 'hw3 computing', 'computing bmi', 'bmi person', 'person average', 'average height', 'height average', 'average mass', 'mass mean', 'mean bmi', 'bmi across', 'across people'], ['one thing', 'thing saw', 'saw instead', 'instead calculating', 'calculating mean', 'mean bmi', 'bmi among', 'among group', 'group humans', 'humans calculated', 'calculated bmi', 'bmi person', 'person average', 'average height', 'height weight'], ['related notable', 'notable many', 'many students', 'students made', 'made mistake'], ['pleasantly level', 'level buy', 'buy competence', 'competence github', 'github especially', 'especially intro'], ['think starting', 'starting students', 'students come', 'come github', 'github familiarity', 'familiarity think', 'think better', 'better job', 'job explaining', 'explaining works'], [], ['one wanted', 'wanted know', 'know r', 'r could', 'could import', 'import data', 'data dell', 'dell data', 'data cube'], ['none recall', 'recall process', 'process semester', 'semester long', 'long project'], ['clear enough', 'enough ideas', 'ideas terms', 'terms started', 'started using'], [], ['wanted know', 'know complex', 'complex text', 'text analysis', 'analysis python'], ['basic statistical', 'statistical principles'], ['want python', 'python examples', 'examples different', 'different codes'], ['seem think', 'think learn', 'learn code', 'code course', 'course reality', 'reality one', 'one ever', 'ever knows', 'knows everything', 'everything na'], [], [], [], [], ['struggling basic', 'basic excel', 'excel operations'], [], [], [], []] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rJRiCxZgRQL",
        "colab_type": "text"
      },
      "source": [
        "# Fitting TFIDF Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBo5DtbFSp6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tfidfModelling(bag):\n",
        "  dictionary = Dictionary(bag)\n",
        "  corpus = [dictionary.doc2bow(doc) for doc in bag]\n",
        "  return(TfidfModel(corpus),corpus,dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NflhA0MX8Sqp",
        "colab_type": "text"
      },
      "source": [
        "#Top 10 tf-idf words \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icq4n8qD90E0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def top10Idf (bag):\n",
        "  model, corpus,dictionary = tfidfModelling(bag)\n",
        "  weights = []\n",
        "  for doc in corpus:\n",
        "    for combo in model[doc]:\n",
        "      weights.append(combo)\n",
        "  #sorted weighted list\n",
        "  weights = sorted(weights, key=lambda w : w[1], reverse=True)\n",
        "  new_weights = []\n",
        "  #make list unique\n",
        "  for word in weights:\n",
        "    if word not in new_weights:\n",
        "      new_weights.append(word)\n",
        "  #print top 10 words based on tfidf\n",
        "  for term_id, weight in new_weights[0:10]:\n",
        "    print(dictionary.get(term_id), weight)\n",
        "  print(\"\\n\") \n",
        "  #return top 10 words based on tfidf\n",
        "  return [(dictionary[pair[0]],pair[1]) for pair in new_weights[0:10]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmZ-Y5vIuix8",
        "colab_type": "text"
      },
      "source": [
        "# Prints the Top 10 for Prof/TA & Students"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djjhC5F9--Il",
        "colab_type": "code",
        "outputId": "0c05cde1-c87b-4192-f628-7448aaee0e13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Topics Covered\n",
        "print(\"Professor/TA Topics:\")\n",
        "top_profta_topics = top10Idf(profta_topics_bag)\n",
        "print(\"Student Topics:\")\n",
        "top_stud_topics = top10Idf(stud_topics_bag)\n",
        "\n",
        "#Struggles Encountered\n",
        "print(\"Professor/TA Struggles:\")\n",
        "top_profta_struggles = top10Idf(profta_struggles_bag)\n",
        "print(\"Student Struggles:\")\n",
        "top_stud_struggles = top10Idf(stud_struggles_bag)\n",
        "\n",
        "#Questions Asked\n",
        "print(\"Professor/TA Questions:\")\n",
        "top_profta_questions = top10Idf(profta_questions_bag)\n",
        "print(\"Student Questions:\")\n",
        "top_stud_questions = top10Idf(stud_questions_bag)\n",
        "\n",
        "#Surprise Questions Asked\n",
        "print(\"Professor/TA Surprise Questions:\")\n",
        "top_profta_surprise = top10Idf(profta_surprise_bag)\n",
        "print(\"Student Surprise Questions:\")\n",
        "top_stud_surprise = top10Idf(stud_surprise_bag)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Professor/TA Topics:\n",
            "github 1.0\n",
            "data 1.0\n",
            "idf 1.0\n",
            "preprocessing 0.9679195717383295\n",
            "acquisition 0.9573122438649556\n",
            "iteration 0.9326446771245245\n",
            "worked 0.852508574458651\n",
            "rows 0.7613948638501783\n",
            "types 0.7311619496050147\n",
            "learning 0.7001663757575962\n",
            "\n",
            "\n",
            "Student Topics:\n",
            "tidy 1.0\n",
            "types 0.966895707705219\n",
            "wrangling 0.9210412666697719\n",
            "projects 0.752896516222731\n",
            "tree 0.7363185426507128\n",
            "language 0.7071067811865475\n",
            "natural 0.7071067811865475\n",
            "bayes 0.7071067811865475\n",
            "naive 0.7071067811865475\n",
            "clustering 0.6666666666666667\n",
            "\n",
            "\n",
            "Professor/TA Struggles:\n",
            "shiny 1.0\n",
            "summarize 1.0\n",
            "programming 1.0\n",
            "wrangling 0.8851521213595946\n",
            "terminology 0.8809504870645746\n",
            "grasping 0.8\n",
            "commandline 0.7071067811865476\n",
            "navigating 0.7071067811865476\n",
            "dataset 0.7071067811865476\n",
            "structuring 0.7071067811865476\n",
            "\n",
            "\n",
            "Student Struggles:\n",
            "class 1.0\n",
            "tidy 0.9087520211324654\n",
            "summary 0.7664881671343302\n",
            "distribution 0.7353011379117274\n",
            "deviation 0.7071067811865476\n",
            "standard 0.7071067811865476\n",
            "material 0.7071067811865476\n",
            "midterm 0.7071067811865476\n",
            "exam 0.7071067811865476\n",
            "reviewing 0.7071067811865476\n",
            "\n",
            "\n",
            "Professor/TA Questions:\n",
            "github 1.0\n",
            "hell 0.9206210278707457\n",
            "model 0.7071067811865476\n",
            "topic 0.7071067811865476\n",
            "python 0.7071067811865475\n",
            "programming 0.7071067811865475\n",
            "create 0.6773039428636662\n",
            "repos 0.6773039428636662\n",
            "columns 0.6666666666666666\n",
            "management 0.646508495532071\n",
            "\n",
            "\n",
            "Student Questions:\n",
            "class 1.0\n",
            "github 1.0\n",
            "midterm 1.0\n",
            "summarize 0.9012952456673496\n",
            "calculate 0.7684606500455718\n",
            "decide 0.7684606500455718\n",
            "markdown 0.7497930241180188\n",
            "related 0.7329872223738487\n",
            "load 0.7205315487005451\n",
            "deviation 0.7071067811865476\n",
            "\n",
            "\n",
            "Professor/TA Surprise Questions:\n",
            "stands 1.0\n",
            "completed 0.7071067811865475\n",
            "survey 0.7071067811865475\n",
            "principles 0.6333351513113149\n",
            "statistical 0.6333351513113149\n",
            "bmi 0.5790105752977722\n",
            "pain 0.5334198428284045\n",
            "data 0.5283488435891197\n",
            "codes 0.5227438351265357\n",
            "examples 0.5227438351265357\n",
            "\n",
            "\n",
            "Student Surprise Questions:\n",
            "class 1.0\n",
            "material 1.0\n",
            "arrays 0.7071067811865475\n",
            "list 0.7071067811865475\n",
            "advanced 0.5180687000586814\n",
            "techniques 0.5180687000586814\n",
            "data 0.5173562945792214\n",
            "data 0.48267466722711555\n",
            "code 0.447213595499958\n",
            "line 0.447213595499958\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prLjqbrXuvk6",
        "colab_type": "text"
      },
      "source": [
        "# Prints the top 10 N-Grams "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elTx9j02H7Bv",
        "colab_type": "code",
        "outputId": "13d09411-a275-4ed0-cf7b-8d95ff7f492a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Topics Covered\n",
        "print(\"Professor/TA Topics:\")\n",
        "top_profta_topics2 = top10Idf(profta_topics_ngrams)\n",
        "print(\"Student Topics:\")\n",
        "top_stud_topics2 = top10Idf(stud_topics_ngrams)\n",
        "\n",
        "#Struggles Encountered\n",
        "print(\"Professor/TA Struggles:\")\n",
        "top_profta_struggles2 = top10Idf(profta_struggles_ngrams)\n",
        "print(\"Student Struggles:\")\n",
        "top_stud_struggles2 = top10Idf(stud_struggles_ngrams)\n",
        "\n",
        "#Questions Asked\n",
        "print(\"Professor/TA Questions:\")\n",
        "top_profta_questions2 = top10Idf(profta_questions_ngrams)\n",
        "print(\"Student Questions:\")\n",
        "top_stud_questions2 = top10Idf(stud_questions_ngrams)\n",
        "\n",
        "#Surprise Questions Asked\n",
        "print(\"Professor/TA Surprise Questions:\")\n",
        "top_profta_surprise2 = top10Idf(profta_surprise_ngrams)\n",
        "print(\"Student Surprise Questions:\")\n",
        "top_stud_surprise2 = top10Idf(stud_surprise_ngrams)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Professor/TA Topics:\n",
            "iteration github 1.0\n",
            "data preprocessing 1.0\n",
            "processing data 0.7783666410061783\n",
            "meetings client 0.7783666410061783\n",
            "students worked 0.7071067811865476\n",
            "worked projects 0.7071067811865476\n",
            "maps practice 0.7071067811865476\n",
            "practice exam 0.7071067811865476\n",
            "gis api 0.7071067811865476\n",
            "maps gis 0.7071067811865476\n",
            "\n",
            "\n",
            "Student Topics:\n",
            "data wrangling 1.0\n",
            "data types 1.0\n",
            "naive bayes 1.0\n",
            "students worked 0.7694780625232275\n",
            "datetime pandas 0.7071067811865475\n",
            "pandas python 0.7071067811865475\n",
            "maps practice 0.7071067811865475\n",
            "practice exam 0.7071067811865475\n",
            "language processing 0.7071067811865475\n",
            "natural language 0.7071067811865475\n",
            "\n",
            "\n",
            "Professor/TA Struggles:\n",
            "grasping github 1.0\n",
            "early introduce 1.0\n",
            "data wrangling 1.0\n",
            "github terminology 1.0\n",
            "navigating commandline 1.0\n",
            "dataset structuring 1.0\n",
            "naive bayes 1.0\n",
            "grasping naive 0.7729084932359204\n",
            "management collaboration 0.7729084932359204\n",
            "presentation skills 0.7071067811865476\n",
            "\n",
            "\n",
            "Student Struggles:\n",
            "standard deviation 1.0\n",
            "tidy data 1.0\n",
            "summary statistics 1.0\n",
            "anything new 1.0\n",
            "joining data 0.7664881671343302\n",
            "formatting issues 0.7071067811865476\n",
            "issues project 0.7071067811865476\n",
            "material midterm 0.7071067811865476\n",
            "new material 0.7071067811865476\n",
            "getting projects 0.7071067811865476\n",
            "\n",
            "\n",
            "Professor/TA Questions:\n",
            "github github 1.0\n",
            "hell github 1.0\n",
            "using github 1.0\n",
            "topic model 1.0\n",
            "programming python 1.0\n",
            "mainly python 0.7071067811865476\n",
            "python programming 0.7071067811865476\n",
            "create github 0.7071067811865476\n",
            "github repos 0.7071067811865476\n",
            "confusion github 0.5773502691896257\n",
            "\n",
            "\n",
            "Student Questions:\n",
            "calculate iqr 1.0\n",
            "standard deviation 1.0\n",
            "z scores 1.0\n",
            "mainly github 1.0\n",
            "r markdown 1.0\n",
            "project related 1.0\n",
            "asked summarize 1.0\n",
            "told case 1.0\n",
            "clarify homework 0.7071067811865476\n",
            "homework assignment 0.7071067811865476\n",
            "\n",
            "\n",
            "Professor/TA Surprise Questions:\n",
            "nothing stands 1.0\n",
            "none remember 1.0\n",
            "completed survey 1.0\n",
            "basic statistical 0.7071067811865475\n",
            "statistical principles 0.7071067811865475\n",
            "basic excel 0.5773502691896257\n",
            "excel operations 0.5773502691896257\n",
            "struggling basic 0.5773502691896257\n",
            "different codes 0.5\n",
            "examples different 0.5\n",
            "\n",
            "\n",
            "Student Surprise Questions:\n",
            "arrays list 1.0\n",
            "new material 1.0\n",
            "cant remember 1.0\n",
            "line code 0.5\n",
            "run specific 0.5\n",
            "shortcut run 0.5\n",
            "specific line 0.5\n",
            "curious similar 0.447213595499958\n",
            "different would 0.447213595499958\n",
            "similar different 0.447213595499958\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9f9DR3wu10h",
        "colab_type": "text"
      },
      "source": [
        "# Function to make Word Cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCn6xiraWY4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wordcloud(dictionary):\n",
        "  wc = WordCloud(background_color='black',max_words=2000,width=1024,height=720,colormap=\"spring\")\n",
        "  wc.generate_from_frequencies(dict(dictionary))\n",
        "  return wc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qu9aXnVu5de",
        "colab_type": "text"
      },
      "source": [
        "# Function to Plot Word Clouds in a Grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-twUpRxgVmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotclouds(wc1,wc2,wc3,wc4,save):\n",
        "  plt.subplot(2,2,1).imshow(wc1,interpolation='bilinear')\n",
        "  plt.title(\"Tfidf\")\n",
        "  plt.axis(\"off\")\n",
        "  plt.subplot(2,2,2).imshow(wc2,interpolation='bilinear')\n",
        "  plt.title(\"NGrams Tfidf\")\n",
        "  plt.axis(\"off\")\n",
        "  plt.subplot(2,2,3).imshow(wc3,interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "  plt.subplot(2,2,4).imshow(wc4,interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "  plt.savefig(save)\n",
        "  plt.clf()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEqb54e-vA-b",
        "colab_type": "text"
      },
      "source": [
        "# Plotting Word Clouds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrDX8ojzcKxx",
        "colab_type": "code",
        "outputId": "b44a4199-8183-4947-d9fc-29d08b47beb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Topics Covered\n",
        "wc1 = wordcloud(top_profta_topics)\n",
        "wc2 = wordcloud(top_profta_topics2)\n",
        "wc3 = wordcloud(top_stud_topics)\n",
        "wc4 = wordcloud(top_stud_topics2)\n",
        "plotclouds(wc1,wc2,wc3,wc4,'cloud_topics')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_FhNXf1hrWx",
        "colab_type": "code",
        "outputId": "f17fe760-3cfd-4c10-d840-43e334d5c659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Questions Asked\n",
        "wc1 = wordcloud(top_profta_questions)\n",
        "wc2 = wordcloud(top_profta_questions2)\n",
        "wc3 = wordcloud(top_stud_questions)\n",
        "wc4 = wordcloud(top_stud_questions2)\n",
        "plotclouds(wc1,wc2,wc3,wc4,'cloud_questions')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvZ-Srsal44D",
        "colab_type": "code",
        "outputId": "3bed108f-b80e-4be9-9eb1-9fb6a6f81a9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Struggles Encountered\n",
        "wc1 = wordcloud(top_profta_struggles)\n",
        "wc2 = wordcloud(top_profta_struggles2)\n",
        "wc3 = wordcloud(top_stud_struggles)\n",
        "wc4 = wordcloud(top_stud_struggles2)\n",
        "plotclouds(wc1,wc2,wc3,wc4,'cloud_struggles')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI33BWtOmPnv",
        "colab_type": "code",
        "outputId": "fc85ed2d-4f0f-4215-efa4-39593ff99893",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Surprise Questions Asked\n",
        "wc1 = wordcloud(top_profta_surprise)\n",
        "wc2 = wordcloud(top_profta_surprise2)\n",
        "wc3 = wordcloud(top_stud_surprise)\n",
        "wc4 = wordcloud(top_stud_surprise2)\n",
        "plotclouds(wc1,wc2,wc3,wc4,'cloud_surprise')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}